# ğŸ“ MLOps APOD Pipeline - Architecture & Design Document

## Executive Summary

The APOD (Astronomy Picture of the Day) ETL Pipeline is a production-ready data ingestion system demonstrating enterprise MLOps best practices. It seamlessly integrates:

- **Workflow Orchestration**: Apache Airflow
- **Data Versioning**: DVC (Data Version Control)
- **Data Persistence**: PostgreSQL
- **Containerization**: Docker & Docker Compose
- **Version Control**: Git/GitHub

## System Architecture

### High-Level Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    External Data Sources                            â”‚
â”‚                                                                      â”‚
â”‚              NASA APOD API (REST Endpoint)                          â”‚
â”‚         https://api.nasa.gov/planetary/apod                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ (JSON Data)
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Apache Airflow Orchestration Layer                      â”‚
â”‚                     (Airflow Scheduler)                             â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Extract    â”‚  Transform  â”‚   Load      â”‚  DVC      â”‚  Git   â”‚  â”‚
â”‚  â”‚   Task      â”‚    Task     â”‚   Task      â”‚  Track    â”‚  Commitâ”‚  â”‚
â”‚  â”‚             â”‚             â”‚             â”‚  Task     â”‚ Task   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â”‚
â”‚        â”‚             â”‚              â”‚            â”‚          â”‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”˜
         â”‚             â”‚              â”‚            â”‚          â”‚
         â–¼             â–¼              â–¼            â–¼          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          Data Storage & Version Control Layer               â”‚
    â”‚                                                             â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚  PostgreSQL  â”‚  â”‚ DVC Metadata â”‚  â”‚   Git Repo    â”‚   â”‚
    â”‚  â”‚   Database   â”‚  â”‚   Storage    â”‚  â”‚   (.dvc files)â”‚   â”‚
    â”‚  â”‚              â”‚  â”‚              â”‚  â”‚               â”‚   â”‚
    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚ data/        â”‚  â”‚  Commits:     â”‚   â”‚
    â”‚  â”‚  â”‚apod    â”‚  â”‚  â”‚ processed/   â”‚  â”‚  - apod_data. â”‚   â”‚
    â”‚  â”‚  â”‚ table  â”‚  â”‚  â”‚ apod_data.   â”‚  â”‚    csv.dvc    â”‚   â”‚
    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚ csv.dvc      â”‚  â”‚  - code       â”‚   â”‚
    â”‚  â”‚              â”‚  â”‚              â”‚  â”‚    changes    â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                                                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          Local File Storage Layer                           â”‚
    â”‚                                                             â”‚
    â”‚  /opt/airflow/data/                                         â”‚
    â”‚  â”œâ”€â”€ raw/                                                   â”‚
    â”‚  â”‚   â””â”€â”€ apod_raw_YYYYMMDD_HHMMSS.json                     â”‚
    â”‚  â””â”€â”€ processed/                                             â”‚
    â”‚      â”œâ”€â”€ apod_data.csv                                      â”‚
    â”‚      â”œâ”€â”€ apod_data.csv.dvc                                  â”‚
    â”‚      â””â”€â”€ apod_transformed.json                              â”‚
    â”‚                                                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Component Architecture

### 1. Extraction Layer (`extract_apod.py`)

**Purpose**: Fetch raw APOD data from NASA API

**Responsibilities**:
- Connect to NASA APOD endpoint
- Retrieve daily picture metadata
- Validate API response
- Save raw JSON to `/opt/airflow/data/raw/`
- Push file path to XCom

**Output**:
```
{
  "date": "2024-01-15",
  "title": "Image Title",
  "url": "https://apod.nasa.gov/apod/image/...",
  "explanation": "Detailed description...",
  "media_type": "image",
  "copyright": "Photographer Name"
}
```

**Error Handling**:
- API connection failures â†’ Task retry
- Invalid response â†’ Task failure
- HTTP errors â†’ Caught and logged

### 2. Transformation Layer (`transform_apod.py`)

**Purpose**: Clean and structure raw data

**Responsibilities**:
- Read raw JSON from previous step
- Extract specific fields of interest
- Create Pandas DataFrame
- Save as CSV and JSON
- Push paths to XCom

**Transformations**:
```
Input Fields  â†’  Output Fields
date          â†’  date
title         â†’  title
url           â†’  url
explanation   â†’  explanation
media_type    â†’  media_type
copyright     â†’  (filtered out)
```

**Output Files**:
1. `/opt/airflow/data/processed/apod_data.csv` - CSV format for easy analysis
2. `/opt/airflow/data/processed/apod_transformed.json` - Structured JSON

### 3. Loading Layer (`load_to_postgres.py`)

**Purpose**: Persist data to multiple destinations

**Responsibilities**:
- Read transformed CSV
- Connect to PostgreSQL
- Create `apod` table if needed
- Insert/update data
- Handle duplicates gracefully

**Database Schema**:
```sql
CREATE TABLE apod (
    date TEXT PRIMARY KEY,
    title TEXT NOT NULL,
    url TEXT,
    explanation TEXT,
    media_type TEXT DEFAULT 'image',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**Conflict Resolution**:
```sql
ON CONFLICT (date) DO UPDATE SET
    title = EXCLUDED.title,
    url = EXCLUDED.url,
    explanation = EXCLUDED.explanation,
    media_type = EXCLUDED.media_type;
```

### 4. Versioning Layer - DVC (`dvc_track_data.py`)

**Purpose**: Version data artifacts independently

**Responsibilities**:
- Initialize DVC (if not already done)
- Track CSV file with `dvc add`
- Create `.dvc` metadata file
- Push path to XCom

**Generated Files**:
- `data/processed/apod_data.csv.dvc` - Metadata file tracking data hash and remote location

**Benefits**:
- Data versioning without bloating Git
- Reproducible data pipelines
- Data lineage tracking

### 5. Git Integration (`git_commit_metadata.py`)

**Purpose**: Link code changes to data versions

**Responsibilities**:
- Configure Git user (if needed)
- Stage DVC metadata files
- Create commit with DVC changes
- Push to remote repository

**Commit Information**:
```
commit: "Update DVC metadata: data/processed/apod_data.csv.dvc"
Author: Airflow Pipeline <airflow@mlops.local>
Content: DVC metadata reflecting current data version
```

## Orchestration Flow (DAG)

### Linear Pipeline Execution

```
Task: extract_apod_data
â”œâ”€ Duration: 2-5 seconds
â”œâ”€ Output: raw JSON file
â””â”€ On Success â†’ transform_apod_data
   â”‚
   â””â”€ Task: transform_apod_data
      â”œâ”€ Duration: 1-2 seconds
      â”œâ”€ Output: CSV + JSON files
      â””â”€ On Success â†’ load_to_postgres
         â”‚
         â””â”€ Task: load_to_postgres
            â”œâ”€ Duration: 2-3 seconds
            â”œâ”€ Output: DB records
            â””â”€ On Success â†’ dvc_track_data
               â”‚
               â””â”€ Task: dvc_track_data
                  â”œâ”€ Duration: 3-5 seconds
                  â”œâ”€ Output: .dvc metadata
                  â””â”€ On Success â†’ git_commit_metadata
                     â”‚
                     â””â”€ Task: git_commit_metadata
                        â”œâ”€ Duration: 2-4 seconds
                        â”œâ”€ Output: Git commit
                        â””â”€ Complete âœ“
```

### Execution Context (XCom)

Tasks communicate via XCom (Cross-Communication):

```
extract_apod_data pushes:
â”œâ”€ raw_apod_path: "/opt/airflow/data/raw/apod_raw_20240115_120000.json"

transform_apod_data pulls raw_apod_path, pushes:
â”œâ”€ transformed_csv_path: "/opt/airflow/data/processed/apod_data.csv"
â”œâ”€ transformed_json_path: "/opt/airflow/data/processed/apod_transformed.json"
â”œâ”€ transformed_df_json: "[{...}]" (DataFrame as JSON)

load_to_postgres pulls transformed_csv_path

dvc_track_data pulls transformed_csv_path, pushes:
â”œâ”€ dvc_file_path: "data/processed/apod_data.csv.dvc"

git_commit_metadata pulls dvc_file_path
```

## Data Flow Diagram

### Step-by-Step Data Journey

```
1. EXTRACTION
   NASA API
       â†“
   requests.get()
       â†“
   JSON Response
       â†“
   Write to file
       â†“
   /opt/airflow/data/raw/apod_raw_*.json

2. TRANSFORMATION
   Read raw JSON
       â†“
   Extract fields
       â†“
   Create DataFrame
       â†“
   Write CSV & JSON
       â†“
   /opt/airflow/data/processed/apod_data.csv
   /opt/airflow/data/processed/apod_transformed.json

3. LOADING
   Read CSV
       â†“
   Connect to Postgres
       â”œâ”€ CREATE TABLE IF NOT EXISTS
       â””â”€ INSERT/UPDATE data
       â†“
   Database updated
       â†“
   PostgreSQL:apod table

4. DVC VERSIONING
   dvc add CSV
       â†“
   Calculate MD5 hash
       â†“
   Create metadata
       â†“
   /opt/airflow/data/processed/apod_data.csv.dvc

5. GIT INTEGRATION
   git config user
       â†“
   git add *.dvc
       â†“
   git commit
       â†“
   git push origin main
       â†“
   GitHub updated
```

## Storage Architecture

### Local File System

```
/opt/airflow/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                              (Auto-created)
â”‚   â”‚   â””â”€â”€ apod_raw_20240115_*.json      (1-10 MB per run)
â”‚   â”‚
â”‚   â””â”€â”€ processed/                        (Auto-created)
â”‚       â”œâ”€â”€ apod_data.csv                 (1-5 KB)
â”‚       â”œâ”€â”€ apod_data.csv.dvc             (~1 KB)
â”‚       â””â”€â”€ apod_transformed.json         (5-10 KB)
â”‚
â”œâ”€â”€ logs/                                  (Airflow logs)
â”‚   â””â”€â”€ apod_etl_pipeline/                (Auto-created)
â”‚       â”œâ”€â”€ extract_apod_data/
â”‚       â”œâ”€â”€ transform_apod_data/
â”‚       â”œâ”€â”€ load_to_postgres/
â”‚       â”œâ”€â”€ dvc_track_data/
â”‚       â””â”€â”€ git_commit_metadata/
â”‚
â””â”€â”€ plugins/
    â””â”€â”€ scripts/
        â”œâ”€â”€ extract_apod.py
        â”œâ”€â”€ transform_apod.py
        â”œâ”€â”€ load_to_postgres.py
        â”œâ”€â”€ dvc_track_data.py
        â””â”€â”€ git_commit_metadata.py
```

### Database Schema

```sql
Database: airflow
â”œâ”€â”€ Table: apod
â”‚   â”œâ”€â”€ PK: date (TEXT)
â”‚   â”œâ”€â”€ title (TEXT, NOT NULL)
â”‚   â”œâ”€â”€ url (TEXT)
â”‚   â”œâ”€â”€ explanation (TEXT)
â”‚   â”œâ”€â”€ media_type (TEXT, DEFAULT 'image')
â”‚   â”œâ”€â”€ created_at (TIMESTAMP, DEFAULT NOW())
â”‚   â””â”€â”€ updated_at (TIMESTAMP, DEFAULT NOW())
â”‚
â”œâ”€â”€ Index: idx_apod_date
â”‚   â””â”€â”€ On: date (for fast lookups)
â”‚
â””â”€â”€ Constraints:
    â””â”€â”€ UNIQUE(date) â†’ ON CONFLICT DO UPDATE
```

### DVC Metadata Structure

```
data/processed/apod_data.csv.dvc (YAML format):

outs:
- path: data/processed/apod_data.csv
  hash: md5
  md5: 1a2b3c4d5e6f7g8h9i0j
  size: 2048
  nfiles: 1

deps: []
```

## Containerization Architecture

### Docker Image Layers

```
Layer 1: apache/airflow:2.9.1-python3.10
â”œâ”€â”€ Python 3.10
â”œâ”€â”€ Airflow 2.9.1
â””â”€â”€ Basic dependencies

Layer 2: System packages
â”œâ”€â”€ git
â”œâ”€â”€ ssh
â”œâ”€â”€ build-essential
â””â”€â”€ libpq-dev

Layer 3: Python packages
â”œâ”€â”€ apache-airflow-providers-postgres
â”œâ”€â”€ pandas
â”œâ”€â”€ requests
â”œâ”€â”€ dvc[ssh]
â”œâ”€â”€ psycopg2-binary
â”œâ”€â”€ sqlalchemy
â””â”€â”€ python-dotenv

Layer 4: Application code
â”œâ”€â”€ airflow/dags/
â”œâ”€â”€ airflow/plugins/scripts/
â””â”€â”€ Configuration files

Final: Service ports
â””â”€â”€ 8080 (Airflow WebUI)
```

### Docker Compose Services

```yaml
services:
  â”œâ”€â”€ postgres
  â”‚   â”œâ”€â”€ Image: postgres:15
  â”‚   â”œâ”€â”€ Ports: 5432:5432
  â”‚   â”œâ”€â”€ Volume: postgres_data:/var/lib/postgresql/data
  â”‚   â””â”€â”€ Env: POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DB
  â”‚
  â””â”€â”€ airflow
      â”œâ”€â”€ Build: ./Dockerfile
      â”œâ”€â”€ Ports: 8080:8080
      â”œâ”€â”€ Volumes: ./airflow:/opt/airflow, ./data:/opt/airflow/data
      â”œâ”€â”€ Depends: postgres (healthcheck)
      â””â”€â”€ Env: AIRFLOW__*, NASA_API_KEY, POSTGRES_*
```

## Technology Stack

| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| **Orchestration** | Apache Airflow | 2.9.1 | DAG scheduling and execution |
| **Data Processing** | Python/Pandas | 3.10/1.3+ | Data transformation |
| **API Client** | Requests | 2.26+ | NASA API calls |
| **Database** | PostgreSQL | 15 | Data warehouse |
| **Python Driver** | psycopg2-binary | 2.9+ | DB connectivity |
| **Versioning** | DVC | 2.0+ | Data artifact versioning |
| **SCM** | Git | 2.25+ | Code versioning |
| **Containerization** | Docker | 20.10+ | Image building |
| **Orchestration** | Docker Compose | 1.29+ | Multi-container setup |

## Security Considerations

### API Key Management
- **Method**: Environment variables
- **Storage**: `.env` file (NOT committed to Git)
- **Rotation**: Support for key rotation via UI update

### Database Security
- **Authentication**: User/password combination
- **Connection**: Can use SSL/TLS (configurable)
- **Separation**: Airflow and app-specific credentials

### Git Integration
- **Authentication**: SSH keys or credentials
- **Scope**: Metadata files only (no sensitive data)
- **Logging**: Git operations logged for audit trail

## Performance Considerations

### Pipeline Duration
- **Typical Total Time**: 15-25 seconds end-to-end
  - Extract: 2-5 sec (API latency)
  - Transform: 1-2 sec (DataFrame creation)
  - Load: 2-3 sec (DB insertion)
  - DVC Track: 3-5 sec (Hash calculation)
  - Git Commit: 2-4 sec (Remote push)

### Scalability Factors
- **Concurrency**: Sequential execution (can be parallelized)
- **Data Volume**: Handles 1-100 MB daily records easily
- **Database**: PostgreSQL can handle millions of records
- **Storage**: Unlimited with DVC remote storage

## Monitoring & Observability

### Airflow UI Monitoring
- DAG execution status
- Task duration tracking
- Real-time log viewing
- XCom value inspection

### Logging
- **Airflow Logs**: `/opt/airflow/logs/apod_etl_pipeline/`
- **Database Logs**: PostgreSQL application log
- **DVC Logs**: Captured in task logs

### Health Checks
- Postgres connection health check
- Task failure retry mechanism
- Alert-on-failure support

## Disaster Recovery

### Data Backup Strategy
- **Raw Data**: Versioned with DVC
- **Processed Data**: Backed up in PostgreSQL
- **Metadata**: Git history for complete lineage

### Recovery Procedures
```
If data is lost:
1. Retrieve version from DVC remote
2. Re-run transform step
3. Reload to PostgreSQL
4. New commit created automatically

If database fails:
1. Restore from backup
2. Re-run load task
3. DVC ensures data consistency
```

## Future Enhancements

### Potential Improvements
- [ ] Parallel loading (simultaneous DB + S3 writes)
- [ ] Data quality checks (validation step)
- [ ] Alerting (email/Slack notifications)
- [ ] Multi-source support (extend beyond APOD)
- [ ] ML model integration (training pipeline)
- [ ] Real-time streaming (Kafka integration)
- [ ] Auto-scaling (Kubernetes deployment)

---

**Architecture Document Version**: 1.0  
**Last Updated**: 2024-01-15  
**Status**: Production Ready âœ“
